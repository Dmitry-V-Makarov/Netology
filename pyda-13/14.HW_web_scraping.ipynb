{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание к лекции \"Основы веб-скрапинга и работы с API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://habr.com/ru/search/'\n",
    "KEYWORDS = ['python', 'парсинг']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://habr.com/ru/post/127584/',\n",
       " 'https://habr.com/ru/post/504900/',\n",
       " 'https://habr.com/ru/post/239081/',\n",
       " 'https://habr.com/ru/post/450834/',\n",
       " 'https://habr.com/ru/company/mailru/blog/420679/',\n",
       " 'https://habr.com/ru/post/519796/',\n",
       " 'https://habr.com/ru/post/500498/',\n",
       " 'https://habr.com/ru/post/273253/',\n",
       " 'https://habr.com/ru/post/446488/',\n",
       " 'https://habr.com/ru/post/317304/']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_links(URL, KEYWORDS):\n",
    "    \n",
    "    all_links = []\n",
    "    \n",
    "    # single keyword\n",
    "    keyword = ('+').join(KEYWORDS)\n",
    "    \n",
    "    params = {\n",
    "        'q': keyword,\n",
    "        'target_type' : 'posts',\n",
    "        'order_by' : 'relevance'\n",
    "    }\n",
    "    \n",
    "    # first page\n",
    "    res = requests.get(URL, params)\n",
    "    time.sleep(0.3)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    post_title = soup.find_all('a', class_='post__title_link')\n",
    "    all_links += list(map(lambda x: x.get('href'), post_title))\n",
    "    \n",
    "    # number of pages\n",
    "    last_page = soup.find_all('a', class_='toggle-menu__item-link toggle-menu__item-link_pagination toggle-menu__item-link_bordered')\n",
    "    last_link = last_page[0].get('href')\n",
    "    total_pages = re.findall(r'\\d\\d', last_link)\n",
    "    total_pages = int(total_pages[0]) + 1\n",
    "    \n",
    "    # all other pages\n",
    "    for i in range(2, total_pages):\n",
    "        res = requests.get(URL + 'page' + str(i) + '/', params)\n",
    "        time.sleep(0.3)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        post_title = soup.find_all('a', class_='post__title_link')\n",
    "        all_links += list(map(lambda x: x.get('href'), post_title))\n",
    "        \n",
    "    return all_links\n",
    "\n",
    "all_links = get_all_links(URL, KEYWORDS)\n",
    "all_links[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kw_search(all_links):\n",
    "    kw_search = pd.DataFrame()\n",
    "    limit = 15\n",
    "    for i, link in enumerate(all_links):\n",
    "        soup = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "        time.sleep(0.3)\n",
    "        date = soup.find('span', class_='post__time')\n",
    "        date = date.get('data-time_published')\n",
    "        title = soup.find('h1', class_='post__title').text\n",
    "        title = title.strip()\n",
    "        text = soup.find('div', class_='post__text').text\n",
    "        text = text.strip()\n",
    "        row = {'date': date, 'title': title, 'text': text}\n",
    "        kw_search = pd.concat([kw_search, pd.DataFrame([row])])\n",
    "        kw_search.reset_index(drop=True, inplace=True)\n",
    "        if i == limit:\n",
    "            break\n",
    "    return kw_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-09-02T10:37Z</td>\n",
       "      <td>Grab — python библиотека для парсинга сайтов</td>\n",
       "      <td>Лет пять-шесть назад, когда я ещё программиров...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-23T12:17Z</td>\n",
       "      <td>Как проанализировать рынок фотостудий с помощь...</td>\n",
       "      <td>В интернете огромное количество открытых данны...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-10-03T08:03Z</td>\n",
       "      <td>Парсим на Python: Pyparsing для новичков</td>\n",
       "      <td>Парсинг (синтаксический анализ) представляет с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-07T11:51Z</td>\n",
       "      <td>Парсинг сайтов — а это вообще легально в России?</td>\n",
       "      <td>По одному из определений парсинг есть синтакси...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-08-24T07:22Z</td>\n",
       "      <td>Новые курсы по Python от Mail.Ru Group</td>\n",
       "      <td>Python — простой, гибкий и популярный язык, пр...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date                                              title  \\\n",
       "0  2011-09-02T10:37Z       Grab — python библиотека для парсинга сайтов   \n",
       "1  2020-07-23T12:17Z  Как проанализировать рынок фотостудий с помощь...   \n",
       "2  2014-10-03T08:03Z           Парсим на Python: Pyparsing для новичков   \n",
       "3  2019-05-07T11:51Z   Парсинг сайтов — а это вообще легально в России?   \n",
       "4  2018-08-24T07:22Z             Новые курсы по Python от Mail.Ru Group   \n",
       "\n",
       "                                                text  \n",
       "0  Лет пять-шесть назад, когда я ещё программиров...  \n",
       "1  В интернете огромное количество открытых данны...  \n",
       "2  Парсинг (синтаксический анализ) представляет с...  \n",
       "3  По одному из определений парсинг есть синтакси...  \n",
       "4  Python — простой, гибкий и популярный язык, пр...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_res = kw_search(all_links)\n",
    "search_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "\n",
    "https://www.avast.com/hackcheck/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breach</th>\n",
       "      <th>site</th>\n",
       "      <th>recordsCount</th>\n",
       "      <th>description</th>\n",
       "      <th>publishDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16794</td>\n",
       "      <td>socialengineered.net</td>\n",
       "      <td>90186</td>\n",
       "      <td>In June 2019, hacking forum SocialEngineered's...</td>\n",
       "      <td>2019-06-27T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3883</td>\n",
       "      <td>brainybatch.com</td>\n",
       "      <td>872730</td>\n",
       "      <td>In January 2017, BrainyBatch Technologies' use...</td>\n",
       "      <td>2017-04-20T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16962</td>\n",
       "      <td>cprewritten.net</td>\n",
       "      <td>1687049</td>\n",
       "      <td>In July 2019, virtual world Club Penguin Rewri...</td>\n",
       "      <td>2019-09-26T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16372</td>\n",
       "      <td>bookmate.com</td>\n",
       "      <td>7775238</td>\n",
       "      <td>In July 2018, Bookmate was allegedly breached....</td>\n",
       "      <td>2019-03-20T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16591</td>\n",
       "      <td>roll20.net</td>\n",
       "      <td>4006528</td>\n",
       "      <td>In January 2019, Roll20 allegedly breached. Th...</td>\n",
       "      <td>2019-03-20T00:00:00Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  breach                  site recordsCount  \\\n",
       "0  16794  socialengineered.net        90186   \n",
       "1   3883       brainybatch.com       872730   \n",
       "2  16962       cprewritten.net      1687049   \n",
       "3  16372          bookmate.com      7775238   \n",
       "4  16591            roll20.net      4006528   \n",
       "\n",
       "                                         description           publishDate  \n",
       "0  In June 2019, hacking forum SocialEngineered's...  2019-06-27T00:00:00Z  \n",
       "1  In January 2017, BrainyBatch Technologies' use...  2017-04-20T00:00:00Z  \n",
       "2  In July 2019, virtual world Club Penguin Rewri...  2019-09-26T00:00:00Z  \n",
       "3  In July 2018, Bookmate was allegedly breached....  2019-03-20T00:00:00Z  \n",
       "4  In January 2019, Roll20 allegedly breached. Th...  2019-03-20T00:00:00Z  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails = ['xxx@gmail.com', 'yyy@gmail.com']\n",
    "\n",
    "params = {\n",
    "    'emailAddresses': emails\n",
    "}\n",
    "headers = {\n",
    "    'Vaar-Version': '0', \n",
    "    'Vaar-Header-App-Product' : 'hackcheck-web-avast'\n",
    "}\n",
    "\n",
    "url = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data?'\n",
    "\n",
    "r = requests.post(url, data = json.dumps(params), headers = headers)\n",
    "# r.json()\n",
    "\n",
    "df = pd.DataFrame(r.json()['breaches'])\n",
    "df = df.transpose().reset_index()\n",
    "df = df.rename(columns={'index': 'breach'})\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
